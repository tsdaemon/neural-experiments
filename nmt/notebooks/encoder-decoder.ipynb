{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "For this tutorial we will use bilingual datasets from [tatoeba.org](https://tatoeba.org/eng/downloads).\n",
    "You can download language pairs data from http://www.manythings.org/anki/, or if there is no pair,\n",
    "as for Ukrainian-German, you can use my script [get_dataset.py](https://github.com/tsdaemon/neural-experiments/blob/master/nmt/scripts/get_dataset.py). It will download raw data from\n",
    "tatoeba and extract a bilingual dataset as a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_lang = 'ukr'\n",
    "target_lang = 'deu'\n",
    "data_dir = 'data/'\n",
    "\n",
    "os.chdir('../')\n",
    "corpus = pd.read_csv(os.path.join(data_dir, '{}-{}.csv'.format(source_lang, target_lang)), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train neural network we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a sequence of numbers. To do this, we will use a vocabulary – a class which will store word indexes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = '<start>'\n",
    "EOS_token = '<end>'\n",
    "UNK_token = '<unk>'\n",
    "PAD_token = '<pad>'\n",
    "\n",
    "SOS_idx = 0\n",
    "EOS_idx = 1\n",
    "UNK_idx = 2\n",
    "PAD_idx = 3\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.index2word = {\n",
    "            SOS_idx: SOS_token,\n",
    "            EOS_idx: EOS_token,\n",
    "            UNK_idx: UNK_token,\n",
    "            PAD_idx: PAD_token\n",
    "        }\n",
    "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
    "\n",
    "    def index_words(self, words):\n",
    "        for word in words:\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            n_words = len(self)\n",
    "            self.word2index[word] = n_words\n",
    "            self.index2word[n_words] = word\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.index2word) == len(self.word2index)\n",
    "        return len(self.index2word)\n",
    "    \n",
    "    def unidex_words(self, indices):\n",
    "        return [self.index2word[i] for i in indices]\n",
    "    \n",
    "    def to_file(self, filename):\n",
    "        values = [w for w, k in sorted(list(self.word2index.items())[5:])]\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('\\n'.join(values))\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, filename):\n",
    "        vocab = Vocab()\n",
    "        with open(filename, 'r') as f:\n",
    "            words = [l.strip() for l in f.readlines()]\n",
    "            vocab.index_words(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenizer functions for you languages to split sentences on words.\n",
    "In this examples I'm using standard [`nltk.tokenize.WordPunctTokenizer`](https://kite.com/python/docs/nltk.tokenize.WordPunctTokenizer) for German and\n",
    "a function `tokenize_words` from package [`tokenize_uk`](https://github.com/lang-uk/tokenize-uk) for Ukrainian.\n",
    "Also, replace input file with your\n",
    "\n",
    "Since there are a lot of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 8 words (that includes punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc.\n",
    "\n",
    "Additionally, you might want to filter out rare words which occur only few times in corpus. This words could not be learned efficiently since there are not enough training examples for them. This will reduce vocabulary size and decrease training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 14044\n",
      "Source vocabulary size: 8497\n",
      "Target vocabulary size: 6138\n",
      "Source: \"апельсини ростуть в теплих країнах .\", target: \"apfelsinen wachsen in warmen ländern .\"\n",
      "Source: \"араби мене переслідують .\", target: \"araber verfolgen mich .\"\n",
      "Source: \"арбітр закінчить гру за дві хвилини .\", target: \"der schiedsrichter wird das spiel in zwei minuten beenden .\"\n",
      "Source: \"астронавти полетіли на місяць в ракеті .\", target: \"die astronauten flogen mit einer rakete zum mond .\"\n",
      "Source: \"африка є колискою людства .\", target: \"afrika ist die wiege der menschheit .\"\n",
      "Source: \"африка — не країна .\", target: \"afrika ist kein land .\"\n",
      "Source: \"афіни — столиця греції .\", target: \"athen ist die hauptstadt griechenlands .\"\n",
      "Source: \"афіни — столиця греції .\", target: \"athen ist die hauptstadt von griechenland .\"\n",
      "Source: \"бабусі подобається дивитися телевізор .\", target: \"oma schaut gerne fernsehen .\"\n",
      "Source: \"багато американців цікавляться джазом .\", target: \"viele amerikaner interessieren sich für jazz .\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from tokenize_uk import tokenize_words\n",
    "import pandas as pd\n",
    "\n",
    "max_length = 10\n",
    "min_word_count = 1\n",
    "\n",
    "tokenizers = {\n",
    "    'ukr': tokenize_words,\n",
    "    'deu': nltk.tokenize.WordPunctTokenizer().tokenize\n",
    "}\n",
    "\n",
    "def preprocess_corpus(sents, tokenizer, min_word_count):\n",
    "    n_words = {}\n",
    "\n",
    "    sents_tokenized = []\n",
    "    for sent in sents:\n",
    "        sent_tokenized = [w.lower() for w in tokenizer(sent)]\n",
    "\n",
    "        sents_tokenized.append(sent_tokenized)\n",
    "\n",
    "        for word in sent_tokenized:\n",
    "            if word in n_words:\n",
    "                n_words[word] += 1\n",
    "            else:\n",
    "                n_words[word] = 1\n",
    "\n",
    "    for i, sent_tokenized in enumerate(sents_tokenized):\n",
    "        sent_tokenized = [t if n_words[t] >= min_word_count else UNK_token for t in sent_tokenized]\n",
    "        sents_tokenized[i] = sent_tokenized\n",
    "\n",
    "    return sents_tokenized\n",
    "\n",
    "def read_vocab(sents):\n",
    "    vocab = Vocab()\n",
    "    for sent in sents:\n",
    "        vocab.index_words(sent)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "source_sents = preprocess_corpus(corpus['text' + source_lang], tokenizers[source_lang], min_word_count)\n",
    "target_sents = preprocess_corpus(corpus['text' + target_lang], tokenizers[target_lang], min_word_count)\n",
    "\n",
    "# Using set to remove duplicates\n",
    "source_sents, target_sents = zip(\n",
    "    *sorted({(tuple(s), tuple(t)) for s, t in zip(source_sents, target_sents)\n",
    "              if len(s) <= max_length and len(t) <= max_length})\n",
    ")\n",
    "\n",
    "source_vocab = read_vocab(source_sents)\n",
    "target_vocab = read_vocab(target_sents)\n",
    "\n",
    "target_vocab.to_file(os.path.join(data_dir, '{}.vocab.txt'.format(target_lang)))\n",
    "source_vocab.to_file(os.path.join(data_dir, '{}.vocab.txt'.format(source_lang)))\n",
    "\n",
    "print('Corpus length: {}\\nSource vocabulary size: {}\\nTarget vocabulary size: {}'.format(\n",
    "    len(source_sents), len(source_vocab.word2index), len(target_vocab.word2index)\n",
    "))\n",
    "examples = list(zip(source_sents, target_sents))[80:90]\n",
    "for source, target in examples:\n",
    "    print('Source: \"{}\", target: \"{}\"'.format(' '.join(source), ' '.join(target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some translation pairs can duplicate each other: one source sentence might has multiple target references. This naturally happen in language, we always have options how to translate a sentence. And this should have considered when we train NMT system, that it is possible to have more than one option of a correct model output. \n",
    "\n",
    "Translation quality metrics like **BLEU** (will be decribed in details later) are designed to use multiple references of a correct translation. To take this into account during evaluation I will combine pairs with identical source into one pair with one source and multiple targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11967"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_to_target = {}\n",
    "for source, target in zip(source_sents, target_sents):\n",
    "    if source in source_to_target:\n",
    "        source_to_target[source].append(target)\n",
    "    else:\n",
    "        source_to_target[source] = [target]\n",
    "    \n",
    "source_sents, target_sents = zip(*source_to_target.items())\n",
    "len(source_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for deep learning experiment is usually split into three parts:\n",
    "* Training data is used for neural network training;\n",
    "* Development data is used to select an optimal training stop point;\n",
    "* Test data is used for final evaluation of experiment performance.\n",
    "\n",
    "We will use 80% of the data as a training set, 6% of the data as a development set and 14% of the data as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "source_length = len(source_sents)\n",
    "inidices = np.random.permutation(source_length)\n",
    "\n",
    "training_indices = inidices[:int(source_length*0.8)]\n",
    "dev_indices = inidices[int(source_length*0.8):int(source_length*0.86)]\n",
    "test_indices = inidices[int(source_length*0.86):]\n",
    "\n",
    "training_source = [source_sents[i] for i in training_indices]\n",
    "dev_source = [source_sents[i] for i in dev_indices]\n",
    "test_source = [source_sents[i] for i in test_indices]\n",
    "\n",
    "training_target = [target_sents[i] for i in training_indices]\n",
    "dev_target = [target_sents[i] for i in dev_indices]\n",
    "test_target = [target_sents[i] for i in test_indices]\n",
    "\n",
    "# Unwrap training examples\n",
    "training_t = []\n",
    "training_s = []\n",
    "for source, tt in zip(training_source, training_target):\n",
    "    for target in tt:\n",
    "        training_t.append(target)\n",
    "        training_s.append(source)\n",
    "        \n",
    "training_source = training_s\n",
    "training_target = training_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses its own format of data – Tensor. A Tensor is a multi-dimensional array of numbers with some type e.g. FloatTensor or LongTensor. Before we can use our training data, we need to convert it into tensors using previously defined word indices.\n",
    "Additionally, we need to add special tokens SOS (start of a sentence) and EOS (end of a sentence) to each sentence.\n",
    "Also, all sentences should have the same length to make possible batch training, therefore we will extend them with token PAD if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def indexes_from_sentence(vocab, sentence):\n",
    "    return [vocab.word2index[word] for word in sentence]\n",
    "\n",
    "def tensor_from_sentence(vocab, sentence, max_seq_length):\n",
    "    indexes = indexes_from_sentence(vocab, sentence)\n",
    "    indexes.append(EOS_idx)\n",
    "    indexes.insert(0, SOS_idx)\n",
    "    # we need to have all sequences the same length to process them in batches\n",
    "    if len(indexes) < max_seq_length:\n",
    "        indexes += [PAD_idx] * (max_seq_length - len(indexes))\n",
    "    tensor = torch.LongTensor(indexes)\n",
    "    return tensor\n",
    "\n",
    "def tensors_from_pair(source_sent, target_sent, max_seq_length):\n",
    "    source_tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
    "    target_tensor = tensor_from_sentence(target_vocab, target_sent, max_seq_length).unsqueeze(1)\n",
    "    return (source_tensor, target_tensor)\n",
    "\n",
    "max_seq_length = max_length + 2  # 2 for EOS_token and SOS_token\n",
    "\n",
    "training = []\n",
    "for source_sent, target_sent in zip(training_source, training_target):\n",
    "    training.append(tensors_from_pair(source_sent, target_sent, max_seq_length))\n",
    "    \n",
    "x_training, y_training = zip(*training)\n",
    "x_training = torch.transpose(torch.cat(x_training, dim=-1), 1, 0)\n",
    "y_training = torch.transpose(torch.cat(y_training, dim=-1), 1, 0)\n",
    "torch.save(x_training, os.path.join(data_dir, 'x_training.bin'))\n",
    "torch.save(y_training, os.path.join(data_dir, 'y_training.bin'))\n",
    "\n",
    "x_development = []\n",
    "for source_sent in dev_source:\n",
    "    tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
    "    x_development.append(tensor)\n",
    "    \n",
    "x_development = torch.transpose(torch.cat(x_development, dim=-1), 1, 0)\n",
    "torch.save(x_development, os.path.join(data_dir, 'x_development.bin'))\n",
    "\n",
    "x_test = []\n",
    "for source_sent in test_source:\n",
    "    tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
    "    x_test.append(tensor)\n",
    "    \n",
    "x_test = torch.transpose(torch.cat(x_test, dim=-1), 1, 0)\n",
    "torch.save(x_test, os.path.join(data_dir, 'x_test.bin'))\n",
    "\n",
    "USE_CUDA = False\n",
    "if USE_CUDA:\n",
    "    x_training = x_training.cuda()\n",
    "    y_training = y_training.cuda()\n",
    "    x_development = x_development.cuda()\n",
    "    x_test = x_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder of a Seq2seq network is a [**Recurrent Neural Network**](https://en.wikipedia.org/wiki/Recurrent_neural_network). A recurrent network can process model a sequence of related data (sentence in our case) using the same set of weights. To do this, RNN uses its output from a previous step as input along with input from the sequence.\n",
    "\n",
    "A naive implementation of RNN is subject to problems with a gradient for long sequences; therefore, I use [**Long-Short Term Memory**](https://en.wikipedia.org/wiki/Long_short-term_memory) as a recurrent module. You should not care about its implementation since it already implemented in PyTorch: [nn.LSTM](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM). This module allows bi-directional sequence processing out-of-the-box – this allows to capture backward relations in a sentence as well as forward relations.\n",
    "\n",
    "Additionally, I use embeddings module to convert word indices into dense vectors. This allows projecting discrete symbols (words) into continuous space which reflects semantical relations in spatial words positions. For this experiment, I will not use pre-trained word vectors and train this representation using machine translation supervision signal. But you may use the pre-trained word embeddings (for Ukrainian [lang-uk](http://lang.org.ua/en/models/#anchor4) project).\n",
    "\n",
    "To not forget the meaning of dimensions for input vectors, I leave comments like `# word_inputs: (batch_size, seq_length)`. Above means that variable `word_inputs` contains a reference to tensor, which has shape `(batch_size, seq_length)`, e.g. it is an array of sequences of length `seq_length`, where array length is `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        init.normal_(self.embedding.weight, 0.0, 0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size, \n",
    "            int(hidden_size/2),  # Bi-directional processing will ouput vectors of double size, therefore I reduced output dimensionality\n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,  # First dimension of input tensor will be treated as a batch dimension\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    # word_inputs: (batch_size, seq_length), h: (h_or_c, layer_n_direction, batch, seq_length)\n",
    "    def forward(self, word_inputs, hidden):         \n",
    "        # embedded (batch_size, seq_length, hidden_size)\n",
    "        embedded = self.embedding(word_inputs) \n",
    "        # output (batch_size, seq_length, hidden_size*directions)\n",
    "        # hidden (h: (batch_size, num_layers*directions, hidden_size),\n",
    "        #         c: (batch_size, num_layers*directions, hidden_size))\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batches):\n",
    "        hidden = torch.zeros(2, self.n_layers*2, batches, int(self.hidden_size/2))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Decoder module is similar to encoder with the difference in that it generates a sequence, therefore it will process inputs one by one; therefore it cannot be bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        init.normal_(self.embedding.weight, 0.0, 0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=n_layers, batch_first=True, bidirectional=False)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this one by one\n",
    "        # embedded (batch_size, 1, hidden_size)\n",
    "        embedded = self.embedding(word_inputs).unsqueeze_(1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "To make sure the Encoder and Decoder models are working (and working together) we'll do a quick test with fake word inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (lstm): LSTM(10, 5, num_layers=2, batch_first=True, bidirectional=True)\n",
      ")\n",
      "torch.Size([1, 3, 10]) torch.Size([4, 1, 5]) torch.Size([4, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "hidden_dim = 10\n",
    "n_layers = 2\n",
    "\n",
    "encoder_test = EncoderRNN(vocab_size, hidden_dim, n_layers)\n",
    "print(encoder_test)\n",
    "\n",
    "# Recurrent network requires initial hidden state\n",
    "encoder_hidden = encoder_test.init_hidden(1)\n",
    "\n",
    "# Test input of size (1x3), one sequence of size 3\n",
    "word_input = torch.LongTensor([[1, 2, 3]])\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "# encoder_outputs: (batch_size, seq_length, hidden_size)\n",
    "# encoder_hidden[0, 1]: (n_layers*2, batch_size, hidden_size/2)\n",
    "print(encoder_outputs.shape, encoder_hidden[0].shape, encoder_hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`encoder_hidden` is a tuple for h and c components of LSTM hidden state. In PyTorch, tensors of LSTM hidden components have a following meaning of dimensions:\n",
    "* First dimension is n_layers*directions, meaning that if we have a bidirectional network, then each layer will store two items in this direction;\n",
    "* Second dimension is a batch dimension\n",
    "* Third dimension is a hidden vector itself\n",
    "\n",
    "The decoder uses single directional LSTM, therefore we need to reshape encoders h and c before sending them into decoder: concatenate all bi-directional vectors into single-direction vectors. This means, that every two vectors along `n_layers*directions` I combine into a single vector, increasing size of hidden vector dimension in two times and decreasing size of the first dimension to `n_layers`, which is two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (lstm): LSTM(10, 10, num_layers=2, batch_first=True)\n",
      ")\n",
      "torch.Size([1, 1, 10]) torch.Size([2, 1, 10]) torch.Size([2, 1, 10])\n",
      "torch.Size([1, 1, 10]) torch.Size([2, 1, 10]) torch.Size([2, 1, 10])\n",
      "torch.Size([1, 1, 10]) torch.Size([2, 1, 10]) torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "decoder_test = DecoderRNN(vocab_size, hidden_dim, n_layers)\n",
    "print(decoder_test)\n",
    "\n",
    "word_inputs = torch.LongTensor([[1, 2, 3]])\n",
    "\n",
    "decoder_hidden_h = encoder_hidden[0].reshape(2, 1, 10)\n",
    "decoder_hidden_c = encoder_hidden[1].reshape(2, 1, 10)\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "\n",
    "for i in range(3):\n",
    "    input = word_inputs[:, i]\n",
    "    decoder_output, decoder_hidden = decoder_test(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "    decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "    print(decoder_output.size(), decoder_hidden_h.size(), decoder_hidden_c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq\n",
    "\n",
    "The logic to coordinate this two modules I have stored in a high-level module `Seq2seq`: it takes care of Encoder-Decoder coordination, a transformation of decoder results into word probability distribution.\n",
    "\n",
    "Also, this module implements two `forward` functions: one for training time and second is for inference. The difference between these two functions is that during training I am using training `y` values (target sentence words) as decoder input; this is called [Teacher Forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). Obviously, during inference, I don't have `y` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, n_layers):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = EncoderRNN(input_vocab_size, hidden_size, self.n_layers)\n",
    "        self.decoder = DecoderRNN(output_vocab_size, hidden_size, self.n_layers)\n",
    "        \n",
    "        self.W = nn.Linear(hidden_size, output_vocab_size)\n",
    "        init.normal_(self.W.weight, 0.0, 0.2)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def _forward_encoder(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        init_hidden = self.encoder.init_hidden(batch_size)\n",
    "        encoder_outputs, encoder_hidden = self.encoder(x, init_hidden)\n",
    "        encoder_hidden_h, encoder_hidden_c = encoder_hidden\n",
    "        \n",
    "        self.decoder_hidden_h = encoder_hidden_h.reshape(self.n_layers, batch_size, self.hidden_size)\n",
    "        self.decoder_hidden_c = encoder_hidden_c.reshape(self.n_layers, batch_size, self.hidden_size)\n",
    "        return self.decoder_hidden_h, self.decoder_hidden_c\n",
    "    \n",
    "    def forward_train(self, x, y):\n",
    "        decoder_hidden_h, decoder_hidden_c = self._forward_encoder(x)\n",
    "        \n",
    "        H = []\n",
    "        for i in range(y.shape[1]):\n",
    "            input = y[:, i]\n",
    "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "            # h: (batch_size, vocab_size)\n",
    "            h = self.W(decoder_output.squeeze(1))\n",
    "            # h: (batch_size, vocab_size, 1)\n",
    "            H.append(h.unsqueeze(2))\n",
    "        \n",
    "        # H: (batch_size, vocab_size, seq_len)\n",
    "        return torch.cat(H, dim=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        decoder_hidden_h, decoder_hidden_c = self._forward_encoder(x)\n",
    "        \n",
    "        current_y = SOS_idx\n",
    "        result = [current_y]\n",
    "        counter = 0\n",
    "        while current_y != EOS_idx and counter < 100:\n",
    "            input = torch.tensor([current_y])\n",
    "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "            # h: (vocab_size)\n",
    "            h = self.W(decoder_output.squeeze(1)).squeeze(0)\n",
    "            y = self.softmax(h)\n",
    "            _, current_y = torch.max(y, dim=0)\n",
    "            current_y = current_y.item()\n",
    "            result.append(current_y)\n",
    "            counter += 1\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "To optimize neural network weights we need to have a model itself and optimizer. Model is already defined and the optimizer is usually available in the NN framework. I use [Adam](http://ruder.io/optimizing-gradient-descent/index.html#adam) from [torch.optim](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = Seq2seq(len(source_vocab), len(target_vocab), 300, 1)\n",
    "optim = Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neural network training is a computationally expensive process, it is better to a train neural network for multiple examples at once. Therefore we need to split our training data on mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def batch_generator(batch_indices, batch_size):\n",
    "    batches = math.ceil(len(batch_indices)/batch_size)\n",
    "    for i in range(batches):\n",
    "        batch_start = i*batch_size\n",
    "        batch_end = (i+1)*batch_size\n",
    "        if batch_end > len(batch_indices):\n",
    "            yield batch_indices[batch_start:]\n",
    "        else:\n",
    "            yield batch_indices[batch_start:batch_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously I mentioned log-lieklyhood function which is used to optimize model parameters; in PyTorch this function is implemented in module `CrossEntropyLoss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate performance of our network we should use a translation quality metric. Standart selection for neural machine translation would be [BLEU](https://en.wikipedia.org/wiki/BLEU) - bilingual evaluation understudy. This metric is proven to have the most correlation with human judgement of translation quality.\n",
    "\n",
    "What is important to understand that BLEU is looking how many common phrases ([n-grams](https://en.wikipedia.org/wiki/N-gram)) are shared between model translation and multiple correct translation references. This could be unigrams (phrases of one word) in BLEU-1, bigrams (two words) in BLEU-2 and so on. Since my dataset is relatively small for neural model, I will use less restrictive BLEU-1 as the main metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def bleu(n):\n",
    "    weights = [1.0/n]*n + [0.0]*(4-n)\n",
    "    return lambda list_of_references, list_of_hypothesis: corpus_bleu(list_of_references, list_of_hypothesis, weights)\n",
    "\n",
    "def accuracy(list_of_references, list_of_hypothesis):\n",
    "    total = 0.0\n",
    "    for references, hypothesis in zip(list_of_references, list_of_hypothesis):\n",
    "        total += 1.0 if tuple(hypothesis) in set(references) else 0.0\n",
    "    return total / len(list_of_references)\n",
    "\n",
    "score_functions = {'BLEU-{}'.format(i):bleu(i) for i in range(1, 5)}\n",
    "score_functions['Accuracy'] = accuracy\n",
    "\n",
    "def score(x, target, desc=None):\n",
    "    scores = {name:0.0 for name in score_functions.keys()}\n",
    "    length = len(target)\n",
    "    list_of_hypothesis = []\n",
    "    for i, pair in tqdm(enumerate(zip(x, target)), \n",
    "                        desc=desc, \n",
    "                        total=length):\n",
    "        x, references = pair\n",
    "        y = model(x.unsqueeze(0))\n",
    "        hypothesis = target_vocab.unidex_words(y[1:-1])  # Remove SOS and EOS from y\n",
    "        list_of_hypothesis.append(hypothesis)\n",
    "        \n",
    "    for name, func in score_functions.items():\n",
    "        score = func(target, list_of_hypothesis)\n",
    "        scores[name] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start to train our model. Each training epoch includes forward propagation, which yields some training results for training target sentences; then `cross_entropy` loss is calculated and `loss.backward()` calculates gradient with respect to the loss for each model parameter. After that, `optim.step()` uses the gradient to adjust model parameters and minimize loss.\n",
    "\n",
    "After each training epoch, the development set is used to evaluate model performance with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. I use `early_stop_counter` to stop the training process if BLEU is not improving for 10 epochs.\n",
    "\n",
    "Module `tqdm` is optional to use, it is a handy and simple way to create a progress bar for a long operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e96bf908ed4ec3ba2ad7b548d6145e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 training is finished, loss: 5.7720\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4404c480b8416699cee05143d985cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 validation is finished.\n",
      "BLEU-1: 0.1350\n",
      "BLEU-2: 0.0120\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "Accuracy: 0.0000\n",
      "The best model is found, resetting early stop counter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccfec68d1304c6ea0021dbb108a1313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import shutil\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "total_batches = int(len(x_training)/BATCH_SIZE) + 1\n",
    "indices = list(range(len(x_training)))\n",
    "            \n",
    "early_stop_after = 10\n",
    "early_stop_counter = 0\n",
    "best_model = None\n",
    "\n",
    "best_score = 0.0\n",
    "scoring_metric = 'BLEU-1'\n",
    "scores_history = []\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # Training\n",
    "    total_loss = 0.0\n",
    "    for step, batch in tqdm(enumerate(batch_generator(indices, BATCH_SIZE)), \n",
    "                            desc='Training epoch {}'.format(epoch+1), \n",
    "                            total=total_batches):\n",
    "        x = x_training[batch, :] \n",
    "        # y for teacher forcing is all sequence without a last element \n",
    "        y_tf = y_training[batch, :-1] \n",
    "        # y for loss calculation is all sequence without a last element \n",
    "        y_true = y_training[batch, 1:]\n",
    "        # (batch_size, vocab_size, seq_length)\n",
    "        H = model.forward_train(x, y_tf)\n",
    "        loss = cross_entropy(H, y_true)\n",
    "        \n",
    "        assert loss.item() > 0\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    loss_history.append(total_loss/total_batches)\n",
    "    print('Epoch {} training is finished, loss: {:.4f}'.format(epoch+1, total_loss/total_batches))\n",
    "    \n",
    "    desc = 'Validating epoch {}'.format(epoch+1)\n",
    "    scores = score(x_development, dev_target, desc=desc)\n",
    "    scores_str = '\\n'.join(['{}: {:.4f}'.format(name, score) for name, score in scores.items()])\n",
    "    scores_history.append(scores)\n",
    "    \n",
    "    print ('Epoch {} validation is finished.\\n{}'.format(\n",
    "        epoch+1, scores_str\n",
    "    ))\n",
    "    \n",
    "    metric = scores[scoring_metric]\n",
    "\n",
    "    # Early Stop\n",
    "    if metric > best_score:\n",
    "        early_stop_counter = 0\n",
    "        print('The best model is found, resetting early stop counter.')\n",
    "        best_score = metric\n",
    "        best_model = model\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print('No improvements for {} epochs.'.format(early_stop_counter))\n",
    "        if early_stop_counter >= early_stop_after:\n",
    "            print('Early stop!')\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
